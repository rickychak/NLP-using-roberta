{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hTDy_shaRs"
      },
      "source": [
        "pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbNDNitRhjoC"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYrG0pwYuR5J"
      },
      "source": [
        "import string\n",
        "\n",
        "train_file = open(\"train.fo1.txt\", \"r\", encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "train_sents = []\n",
        "for i in train_file:\n",
        "  train_sents.append(i)\n",
        "train_data  = \" \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sazZWw7_2PwP"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcED0yHQ2SbC"
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(\"train.fo1.txt\", vocab_size = 33_278, min_frequency = 2, \n",
        "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8GC_dEd3gB1"
      },
      "source": [
        "tokenizer.save_model('tokenizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqC05V1x39z1"
      },
      "source": [
        "from transformers import RobertaTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZXHN2d5Ejw"
      },
      "source": [
        "#config_file = RobertaTokenizer.from_pretrained('tokenizer')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('tokenizer', max_len=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAUUcEAt7QxO"
      },
      "source": [
        "tokens = tokenizer('as')\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPWL72BN9-lA"
      },
      "source": [
        "#@title 預設標題文字\n",
        "#@title labels == input_ids\n",
        "#@title input_ids -> MLM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5Tv6TrCY7E"
      },
      "source": [
        "import torch\n",
        "#15% of the words are being masked\n",
        "def mLm(tensor):\n",
        "  rand = torch.rand(tensor.shape)\n",
        "  mask_arr = (rand < 0.15) * (tensor > 2)\n",
        "  for i in range(tensor.shape[0]):\n",
        "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    tensor[i, selection] = 4\n",
        "  return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phHBr2chHA3n"
      },
      "source": [
        "input_ids = []\n",
        "mask = []\n",
        "labels = []\n",
        "\n",
        "train_sents = []\n",
        "with open(\"train.fo1.txt\", \"r\", encoding = \"utf8\") as file:\n",
        "  sents = file.read().split('\\n')\n",
        "  sample = tokenizer(sents, max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n",
        "  labels.append(sample.input_ids)\n",
        "  mask.append(sample.attention_mask)\n",
        "  input_ids.append(mLm(sample.input_ids.detach().clone()))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKWoW4-cNHbU"
      },
      "source": [
        "input_ids = torch.cat(input_ids)\n",
        "mask = torch.cat(mask)\n",
        "labels = torch.cat(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbYp89oNNk4J",
        "outputId": "9aee4d82-b091-41b8-d9dd-3b029d164efb"
      },
      "source": [
        "input_ids[0][:10]\n",
        "labels[0][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,  3221, 27251,   546,  7757,   474,   539,   225,     3,  7775])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BOegfp4PNrZ"
      },
      "source": [
        "encodings = {\n",
        "    'input_ids': input_ids,\n",
        "    'attention_mask': mask,\n",
        "    'labels': labels\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0F-BYHBPY19"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __len__(self):\n",
        "    return self.encodings['input_ids'].shape[0]\n",
        "  def __getitem__(self, i):\n",
        "    return { key: tensor[i] for key, tensor in self.encodings.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqloHowPwQl"
      },
      "source": [
        "dataset = Dataset(encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t4heoX5RDkV"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPTTbuAcROz4"
      },
      "source": [
        "from transformers import RobertaConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqr1AFXFSVuM"
      },
      "source": [
        "config = RobertaConfig(\n",
        "    vocab_size = 33_278,\n",
        "    max_position_embeddings = 514,\n",
        "    hidden_size = 768,\n",
        "    num_attention_heads = 12,\n",
        "    num_hidden_layers = 6,\n",
        "    type_vocab_size = 1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHyjmLeGT3RF"
      },
      "source": [
        "from transformers import RobertaForMaskedLM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix_KdjrYT44M"
      },
      "source": [
        "model = RobertaForMaskedLM(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkeH1lXnVUSP"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38Uq-lhVhZq"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1-hczmYVmC8"
      },
      "source": [
        "from transformers import AdamW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tqvohvCVoBZ"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0jrRTzfVvCX"
      },
      "source": [
        "optim = AdamW(model.parameters(), lr=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YwAVkxbXsF1"
      },
      "source": [
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prImVriEV6Is"
      },
      "source": [
        "epochs = 2\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IaoMW22V8aC"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#loop = tqdm(enumerate(iter(dataloader), 0), leave=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loop = tqdm(dataloader, leave= True)\n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids, attention_mask = mask, labels=labels)\n",
        "\n",
        "  #loss = criterion(outputs, labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch}')\n",
        "    loop.set_postfix(loss = loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjTDsum9fa_s"
      },
      "source": [
        "model.save_pretrained('./tokenizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3R3uBJNesns"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}